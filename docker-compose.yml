services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 30s
      timeout: 10s
      retries: 3
    mem_limit: 512m
    mem_reservation: 256m

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_HEAP_OPTS: "-Xmx512m -Xms256m"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9092"]
      interval: 30s
      timeout: 10s
      retries: 3
    mem_limit: 1g
    mem_reservation: 512m

  minio:
    image: minio/minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
    mem_limit: 512m
    mem_reservation: 256m

  postgres:
    image: postgres:15
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: crop_disease_ml
      POSTGRES_USER: ml_user
      POSTGRES_PASSWORD: ml_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./services/crop-disease-service/database/init.sql:/docker-entrypoint-initdb.d/init.sql
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ml_user -d crop_disease_ml"]
      interval: 30s
      timeout: 10s
      retries: 3
    mem_limit: 1g
    mem_reservation: 512m

  # Spark Cluster
  spark-master:
    image: apache/spark:3.5.0-python3
    container_name: spark-master
    hostname: spark-master
    ports:
      - "7077:7077"  # Spark Master port
      - "8080:8080"  # Spark Master UI
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "--host", "spark-master", "--port", "7077", "--webui-port", "8080"]
    volumes:
      - spark_shared_data:/opt/spark/shared-data
    restart: unless-stopped
    mem_limit: 1g
    mem_reservation: 512m
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3

  spark-worker-1:
    image: apache/spark:3.5.0-python3
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      spark-master:
        condition: service_healthy
    ports:
      - "8081:8081"  # Worker 1 UI
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_PORT=7078
      - SPARK_WORKER_WEBUI_PORT=8081
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077", "--cores", "2", "--memory", "2g", "--webui-port", "8081"]
    volumes:
      - spark_shared_data:/opt/spark/shared-data
    restart: unless-stopped
    mem_limit: 3g
    mem_reservation: 2g

  spark-worker-2:
    image: apache/spark:3.5.0-python3
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      spark-master:
        condition: service_healthy
    ports:
      - "8082:8082"  # Worker 2 UI
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_PORT=7079
      - SPARK_WORKER_WEBUI_PORT=8082
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077", "--cores", "2", "--memory", "2g", "--webui-port", "8082"]
    volumes:
      - spark_shared_data:/opt/spark/shared-data
    restart: unless-stopped
    mem_limit: 3g
    mem_reservation: 2g


  weather-producer:
    build: ./services/weather-service/producer
    volumes:
      - ./services/weather-service/producer/producer.py:/app/producer.py
      - ./services/weather-service/producer/entrypoint.sh:/app/entrypoint.sh
    working_dir: /app
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - API_KEY=${WEATHER_API_KEY}
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - TIMEZONE=Europe/Rome
      - DEFAULT_LOCATION=${DEFAULT_LOCATION}
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m



  sensor-producer:
    build: ./services/sensor-service/producer
    volumes:
      - ./services/sensor-service/producer/entrypoint.sh:/app/entrypoint.sh
    working_dir: /app
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - TIMEZONE=Europe/Rome
      - ANOMALY_PROBABILITY=0.5 # TODO: change with env variable
    restart: unless-stopped
    mem_limit: 256m
    mem_reservation: 128m



  satellite-producer:
    build: ./services/satellite-service/producer
    volumes:
      - ./services/satellite-service/producer/producer.py:/app/producer.py
      - ./services/satellite-service/producer/entrypoint.sh:/app/entrypoint.sh
    working_dir: /app
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_healthy
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
      MINIO_HOST: minio
      MINIO_PORT: 9000
      COPERNICUS_CLIENT_ID: ${COPERNICUS_CLIENT_ID}
      COPERNICUS_CLIENT_SECRET: ${COPERNICUS_CLIENT_SECRET}
      # Configurazione area geografica (default: Verona) 
      # TODO: change with env variables
      BBOX_MIN_LON: ${BBOX_MIN_LON:-10.894444}
      BBOX_MIN_LAT: ${BBOX_MIN_LAT:-45.266667}
      BBOX_MAX_LON: ${BBOX_MAX_LON:-10.909444}
      BBOX_MAX_LAT: ${BBOX_MAX_LAT:-45.281667}
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m



  spark-data-lake-service:
    build: ./services/storage-service
    volumes:
      - ./services/storage-service:/app
      - spark_checkpoints:/app/spark-checkpoints
      - spark_shared_data:/opt/spark/shared-data
    working_dir: /app
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_healthy
      spark-master:
        condition: service_healthy
      spark-worker-1:
        condition: service_started
      spark-worker-2:
        condition: service_started
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
      TIMEZONE: Europe/Rome
      SPARK_PARALLELISM: 4
      # Spark Cluster Configuration
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_DRIVER_HOST: spark-data-lake-service
      SPARK_DRIVER_PORT: 4040
      SPARK_DRIVER_MEMORY: 1g
      SPARK_EXECUTOR_MEMORY: 1g
      SPARK_EXECUTOR_CORES: 2
      SPARK_CORES_MAX: 2
      # Data lake paths (override default s3a URIs used in code)
      BRONZE_PATH: s3a://bronze/
      SILVER_PATH: s3a://silver/
    ports:
      - "4040:4040"  # Spark UI
    command: ["python", "main_service.py"]
    restart: unless-stopped
    mem_limit: 2g
    mem_reservation: 1g

  crop-disease-service:
    build: ./services/crop-disease-service
    volumes:
      - ./services/crop-disease-service:/app
      - spark_shared_data:/opt/spark/shared-data
    working_dir: /app
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      spark-master:
        condition: service_healthy
      spark-worker-1:
        condition: service_started
      spark-worker-2:
        condition: service_started
      spark-data-lake-service:
        condition: service_started
    environment:
      # Crop Disease Service Configuration
      CROP_DISEASE_SERVICE_HOST: 0.0.0.0
      CROP_DISEASE_SERVICE_PORT: 8000
      
      # PostgreSQL Configuration
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: crop_disease_ml
      POSTGRES_USER: ml_user
      POSTGRES_PASSWORD: ml_password
      
      # MinIO Configuration
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
      
      # Spark Cluster Configuration
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_DRIVER_HOST: crop-disease-service
      SPARK_DRIVER_PORT: 4042
      SPARK_DRIVER_MEMORY: 512m
      SPARK_EXECUTOR_MEMORY: 512m
      SPARK_EXECUTOR_CORES: 1
      SPARK_CORES_MAX: 1
      
      # Kafka Configuration
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      
      # Data Lake Paths (for reference only - not used by alert service)
      GOLD_PATH: s3a://gold/
      
      # Timezone
      TIMEZONE: Europe/Rome
      
      # Alert Processing Configuration
      ALERT_CHECKPOINT_LOCATION: /tmp/alerts-checkpoints
      
    ports:
      - "8000:8000"  # Alert Service API
      - "4042:4042"  # Spark UI for Alert Service
    restart: unless-stopped
    mem_limit: 2g
    mem_reservation: 1g

  dashboard:
    build: ./services/dashboard
    volumes:
      - ./services/dashboard:/app
    working_dir: /app
    depends_on:
      crop-disease-service:
        condition: service_started
    environment:
      # Dashboard Configuration
      STREAMLIT_SERVER_PORT: 8501
      STREAMLIT_SERVER_ADDRESS: 0.0.0.0
      
      # API Configuration
      CROP_DISEASE_API_URL: http://crop-disease-service:8000/api/v1
      
      # Timezone
      TIMEZONE: Europe/Rome
      
      # Kafka
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
    ports:
      - "8501:8501"  # Streamlit Dashboard
    restart: unless-stopped
    mem_limit: 512m
    mem_reservation: 256m

  ml-anomaly-service:
    build: ./services/ml-anomaly-service
    container_name: ml-anomaly-service
    hostname: ml-anomaly-service
    volumes:
      - ./services/ml-anomaly-service:/app
      - ml_models:/app/models
      - ml_checkpoints:/tmp/ml-checkpoints
      - spark_shared_data:/opt/spark/shared-data
    working_dir: /app
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      spark-master:
        condition: service_healthy
      spark-worker-1:
        condition: service_started
      spark-worker-2:
        condition: service_started
      spark-data-lake-service:
        condition: service_started
    environment:
      # ML Service Configuration
      ML_SERVICE_HOST: 0.0.0.0
      ML_SERVICE_PORT: 8002
      
      # PostgreSQL Configuration
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: crop_disease_ml
      POSTGRES_USER: ml_user
      POSTGRES_PASSWORD: ml_password
      
      # MinIO Configuration
      MINIO_ENDPOINT: minio:9000
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
      
      # Kafka Configuration
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      
      # Spark Cluster Configuration - Workers now have ML dependencies
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_DRIVER_HOST: ml-anomaly-service
      SPARK_DRIVER_PORT: 4041
      SPARK_DRIVER_MEMORY: 512m
      SPARK_EXECUTOR_MEMORY: 512m
      SPARK_EXECUTOR_CORES: 1
      SPARK_CORES_MAX: 1
      
      # Python compatibility - Use Python with installed packages
      PYSPARK_PYTHON: /usr/local/bin/python3
      PYSPARK_DRIVER_PYTHON: /usr/local/bin/python3
      
      # Timezone
      TIMEZONE: Europe/Rome
      
    ports:
      - "8002:8002"  # ML Service API
      - "4041:4041"  # Spark UI (different port to avoid conflict)
    restart: unless-stopped
    mem_limit: 2g
    mem_reservation: 1g

volumes:
  minio_data:
  spark_checkpoints:
  postgres_data:
  ml_models:
  ml_logs:
  ml_checkpoints:
  alert_logs:
  spark_shared_data: